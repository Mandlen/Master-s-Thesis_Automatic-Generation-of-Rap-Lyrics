\label{chap:discussion}
\section{Discussion}

This section covers the findings from the research and experients outlined in the paper, commenting on and analysing their nuances and importance. It also presents discussions regarding a number of topics, both directly related to the findings, but also more generally to the fields of computational creativity, NLP, ML etc.

\subsection{Tuning and Evaluation}
\label{sec:tuning+eval-discussion}

First we will go the over findings from process of tuning the decoding algorithm parameters and generating + evaluating lyrics, both intra- and inter-modelly.

Drawing upon our research, there are a number of interesting takeaways. Firstly, it was evident from the parameter tuning of our decoding algorithms, that it is a topic within NLG that needs more attention. As a topic, the subject of parameter tuning is one often left to either a footnote or a remark such as “we tuned the parameters optimally, and these are their values”. Considering the vast differences between different configurations we saw in \cref{chap:tuning-params}, and the work it took to narrow the field down to a fitting number of plausible configurations, this approach seems misguided. By walking through the process methodically, journaling the findings along the way, one is able to share the insight gained along the way, to the benefit of the field, adding important data to the literature, potentially either underpinning or undermining the status-quo. Furthermore, it benefits the paper in question, by avoiding the pitfall of just embracing said status-quo, despite not knowing whether it is the optimal approach for the specific task at hand. 

%As such, in this paper, I hope to have successfully showcased, not only the benefits of extensively optimizing parameters, particularly in the case of decoding algorithms, but also in journaling and including your findings.
The effectiveness of this approach was further highlighted by the parameter tuning of the \hyperref[sec:tuningnucleus]{nucleus sampling algorithm}, in which 4 stages of preliminary tuning was required to settle on the ideal parameters which, even then, could only be narrowed down to 3 configurations. \\\\\\

Even if one were to disregard the accuracy with which the parameters were tuned/chosen, it still functions to highlights the importance of the exploitative process, as documenting one's mistakes is one of the pillars that \textbf{makes} science what it is. Also, this process is certainly not limited to paramter tuning, and should include exploration of the use of different metrics, trying to gauge which might be most suitable for determining the appropriate parameters, an area which is especially ripe for exploration, within the field of computational creativity.

In examination of the metrics in \cref{fig:extended_metrics}, we found a large discrepancy between the \textbf{average real word length} of the best performing configuration with 3.624 chars/w and the lyrics dataset with 5.17 chars/w. This can likely be attributed to our model’s capacity and/or willingness to spell longer words, as they present both a greater level of uncertainty, as well as a higher risk of misspelling. It is also likely a product of tuning for baseline level consistency with regards to producing real words/language, over focusing on pure novelty. Had I instead chosen configurations with different parameter values (higher \textit{temperatures}/\textit{k-values}, or lower \textit{p-values}), the result would have undoubtedly been a higher average real word length. As mentioned, however, this would have also resulted in a decrease in the number of real worse, as large parts of the lyrics would be, to use a technical term, gibberish.

Testing the average rhyme density of the parameter configurations in \cref{tab:decoding-configs-rd}, I found that our best performing models in terms of generating novel, coherent English text, came in dead last in average rhyme density. However, along with these results come a number of caveats. For one, the primary limiting factors for the deterministic coding schemes, repetitions, is unlikely to inhibit them in any meaningful way, when it comes to producing a high density of rhymes. Furthermore, these configurations excelled in producing correctly spelled words in sentences with correct syntax, despite many of these being repeated very often, something which is not accounted for looking at \textit{just} RD. Instead, we might even expect this mirroring of commonly seen lines from RL dataset to be \textit{beneficial} for RD, given that commonly observed rhymes from the dataset would, in theory, be repeated more often, than with other decoding configurations.

Also, as was evident from the inclusion of the \textbf{number of non-words} metric, the impressive performance of the \textbf{top-k} configurations was at least partly undermined by their tendency to generate non-words which, while rhyming, would be largely unserviceable
in any real-world context, maybe outside of generating rap songs for aliens.

Another possible explanation for the comparative under-performance of the nucleus sampling scheme, can be found in its greater number of unique words generated, which seems to suggest a higher level of novelty in the generated lyrics, which might have been costly terms of rhyme generation.

In our tests concerning the influence of outliers and the spectrum of performance in \cref{sec:outliers+performance-spectrum}, we found that there was no significant discrepancy in terms of some schemes being top-/bottom heavy. This showed that that the total average rhyme densities from \cref{tab:decoding-configs-rd} accurately represented the rhyming abilities of various configurations. While no surprise, the tests did, however, showcase that in filtering out the worst verses, only sampling from, say, the best 10/20\%, all configurations were able to achieve substantially better results than on average. It is still nonetheless important to keep in mind that these verses, despite their impressive RD at the top end, are littered with a multitude of errors of various kinds (repetition, non-words, syntax etc.), even in our most linguistically proficient configurations using \textit{nucleus sampling}. Still, this approach might prove interesting in terms of real-world applications, as while a lot of the data might be unsuitable due to errors, it might still be useful if one filters out the worst performers.

In \cref{fig:non-words} and \cref{fig:gp_non-words} we saw that our GP model did substantially worse than our LR model, across the board. There are a number of explanations as to why our GP model was performing so comparatively poorly. Firstly, our GP model managed to reach a categorical cross-entropy score of 1.3924 which is quite a bit higher, and therefore \textit{worse}, than the 1.3242 reached by our RL model. An explanation for this might simply be that the GP model wasn't trained for a sufficient amount of time, and longer training times might prove advantageous. While this seems unlikely, as the model was trained for a similar number of epochs ($RL=232, GP=204$), it is nevertheless a possibility. It might also be due to the dataset in question being harder to generalize. This seems seems plausible, given that the Wikipedia articles' higher diversity, as suggested by its substantially larger vocabulary size ($RL=38.7$k, $GP=78.6$k). However, as mentioned, the best performing configurations do still provide a decent baseline of how well our RL model has been able to imitate rap lyrics specifically, vs. another general-purpose model.

\subsection{Limitations and Prospects for Future Work}
\label{sec:limitation+future-work}

Outside of the ideas mentioned above, there are a number of other interesting avenues for exploration, in building upon the findings in this paper. Regarding pre-processing, it would be interesting to see how removing “noisy” text would affect the model, specifically things such as "spoken", i.e. "not sung", parts of songs such as intros and outros. It might also be interesting to apply some sort of spellchecker to avoid/amend misspellings, which might in turn make the \textit{non-words} metric more accurate and reliable.

Looking at the number of \hyperref[fig:avg-bigrams]{duplicate bigrams} and especially \hyperref[fig:combined-param-eval]{duplicate lines}, also highlights a potential flaw with our pre-processing. In processing the data, I decided to retain all choruses as a means of preserving as much data as possible, primarily due to the hope that the increased amount of repetition might positively influence the model's ability to discern and replicate rhymes/rhyming. The decision was also, however, informed by the limitations of our dataset, as the songs were not annotated for song structure, which meant that the detection of choruses would be difficult and likely inaccurate. This would, in turn, have resulted in a severely reduced dataset which might even still contain some choruses, depending on the success with which I would be able to detect them, which did not seem like a good idea, especially considering the already limited size of the dataset for our character-based approach. However, if one were to remove the duplicate choruses successfully, especially if using a larger dataset which might offset the issue of over-reducing its size, it might alleviate some of the repetition issues, both on the level of bigrams and duplicate lines. While it is unclear whether this would negatively affect the model's ability to produce rhymes, it is nevertheless an interesting path of inquiry.

Our results also highlight the importance of looking at a wide spectrum of metrics, when judging the generated text. In \cref{tab:repetition-issues} we saw two different types of repetition errors, which were each highlighted by a different metric, namely repeating \textbf{duplicate lines/v} for the greedy search example and \textbf{average number of duplicate bigrams/v} for the beam search example. Had either of these metrics been excluded, this repetition might not have been as thoroughly documented, and consequently the decoding algorithm would have been overestimated. This also highlights the importance of including metrics for both text legibility as well as goal-specific ones such as rhyme density.

If one were to attempt a project in which the focus would instead be on generating full length rap songs, an approach which might be interesting to explore would be to add yet more structure to the lyrics by introducing tokens such as \textless{}endChorus\textgreater{} and/or \textless{}endBridge\textgreater{}.

It might also be interesting to attempt to somehow incorporate phonetic data into the text itself, in an attempt to improve the model’s ability to rhyme. This might be done in parallel with phonetic vocabularies, either during decoding, or potentially even during training, if using a word-based model.

\subsubsection{Extending Evaluation and Decoding}
\label{sec:extending-evaluation}

One of the areas we might look to as to be able to better determine the quality of our language model is advanced evaluation metrics. As previously mentioned, the metrics I employed in this paper were generally fairly rudimentary, meant to give us a general idea of differences in performance between the different decoding methods and parameter configurations, as well as model vs. rap lyrics, in the case of rhyme density. One of the most interesting and emerging forms of NLG evaluation metrics is embedding-based metrics, such as BERT-score \cite{ZhangTianyi2019BETG}. BERT-score functions by computing a similarity score for each token in the source set vs. each token in the reference set. However, BERT sets itself apart in its vast number of machine translation and image captioning systems, upon which it draws evaluations, resulting in model selection criteria which better conform to human selection performance. The reason that BERT might be especially interesting for the field of computational creativity, is the aforementioned issue with quantifying \textit{why} we like a particular piece of art, be it rap, poetry or a tagged urinal. An example of how it might have been useful within this current context can be seen by our inability to consistently use a metric like \hyperref[para:avg-unique-words/v]{AUW (Average Unique Words/verse)}, due to its values denoting wildly different patterns, as can be seen in Table 2. The reason that BERT might alleviate part of this issue, is due to its use of Inverse Document Frequency (IDF), which is a measure that rewards matches for words that appear in fewer contexts, in our case, fewer verses. An interesting thing to note about implementations such as BERT score is that, while it might help us evaluate NLG better than previously, producing results which are rated more highly by humans, it does so without actually interacting with the issue at hand; understanding creativity. This, however, is a struggle with \textit{all} neural network models; the monkey’s paw of machine learning. You create a tools that produces wonderful results, but due to the extreme complexity of the non-linear combinations within the model, you none the wiser with regards to \textit{how} it achieved those results.

For future work, it would be interesting to implement an even larger range of decoding techniques, the main ones missing from our examination being the different varieties of beam search, namely those implementing a task-specific diversity scoring function. While beam search is prone to repetitive generation, as shown both here and in previous studies \cite{Welleck2019Unlikelyhood} \cite{HoltzmanAri2019TCCo}, it would nonetheless be interesting to experiment with different diversity scoring functions, especially in combination with other emerging techniques such as the aforementioned unlikelihood training \cite{Welleck2019Unlikelyhood}.

Both lyrics generation specifically and computational creativity as a whole is a rapidly developing field which is in constant flux, finding new and interesting avenues of enquiry. In Vaswani et al. (2017) \cite{VaswaniAshish2017AIAY} they present the “transformer” model, a new approach to neural networks architecture, based on attention mechanisms. This type of model sets itself apart in that it makes way with recurrence and convolutions entirely, which enables it to employ parallelisation, significantly increasing the training speed. Their model manages to outperform the state-of-the-art of multiple translation tasks by a significant margin and appears a very promising technique for future examination, making its implementation very interesting in all contexts which have previously used RNNs.

\subsection{Expanding Creativity and Creative Content}
\label{sec:expanding-creativity+creative-content}

As briefly mentioned in \cref{chap:tuning-params}, comparing human- and machine generated outputs open up for a lot of questions regarding fairness of comparison. Setting aside, for a moment, these questions of equality and fairness, in comparing only the top X\% of the generated lyrics, this approach makes far better use of the inherent advantage of automatic systems, being that they are able to create far larger amounts of content in far less time. Even if we consider Eminem's joking claim that he spent the same amount of time writing his song \textit{Rap God}, as it took to perform (6 minutes and 3 second) \cite{EminemYTShortRapGod}, it pales in comparison with the speed of modern systems, not to mention their ability to run 24/7 if required. And this ability, throwing copious amounts of math-solving abilities at a problem, is one which machines have employed to outperform humans, ever since invention of the \textit{Turing Machine} by the mathematician of the same name, in 1936. And now, as has been shown time and time again, the battlefield has moved from chessboard, where Kasparov unsuccessfully fought against DeepBlue in 1997, to the canvases, the books pages, the newspapers and, more recently, the ear drums.

\subsubsection{Emergence of Pseudo-Creative Systems}
\label{sec:pseudo-creative-system}

Within the last few decades, there has been a plethora of new semi- and fully “creative” systems, referring to systems which are able to perform tasks that has historically thought to require human ingenuity, such as songwriting, painting, conversing, performing summarizing, etc.

%A promising start to the inquiry into all of the above questions and more can be found in \cite{LambCarolyn2018ECCA}.

\subsubsection{Human Creativity and Language Patterns}
\label{sec:creativity+language-patterns}

An interesting disconnect between generative language learning models and real human language is that, while language models almost always attempt to maximize likelihood / minimize loss, at least during training, this does not seem like the metric on which human communication is based. While approaches have been proposed, and are starting to be implemented, in attempts to fix this shortcoming \cite{Welleck2019Unlikelyhood} \cite{HoltzmanAri2019TCCo}, it is quite impressive how well MLE models have been able to imitate language, despite this apparent disconnect between the objectives.

In many ways, our attempts to create language models that parrot human speech has created models which due just that. The effectiveness of large, comprehensive MM models such as GPT-2 and GPT-3 are akin to the effectiveness of the very best impersonators. By relying on the most common “stereotypes” of certain people, or peoples, they are able to imitate their targets with extreme precision, especially on the surface. As you dive deeper, however, as we have seen in the previously, these imitations, even from the very best proverbial parrots, do not tell the whole story; in actuality, they are just scratching the surface. While this metaphor might have gone a little far in terms of discrediting the monumentous achievements we have seen in recent years, it is nevertheless befitting of one of the most exciting problems in within the field of LM today. While various metrics for NLG provide a glimpse into the effectiveness of our model, it hardly gives us any extensive idea as to the \textit{real} quality of our generated lyrics, to mention the “creative” ability of the model. As mentioned in \cref{sec:eval-of-generated-text}, evaluating generated lyrics if far from an easy task, and significant work still has to be done in the hopes of creating a comprehensive evaluation framework. One of the most fascinating things in working with computational creativity, is the way it makes one ponder over words whose definitions and meanings we might previously have thought trivial. On a fundamental level, we all believe we know what \textit{originality} and \textit{creativity} is, but now that we are trying to replicate it, we are suddenly puzzled by the issue of how to quantify it.

If we take, for instance, the model we have trained in this paper, whose job it is the maximize the likelihood, or rather minimize the unlikelihood, of correctly predicting the next token. If the model produces a novel and stimulating set of verses, is the model being creative? Yes? What about a simple Markov-chain model then, whose rules are even simpler and more transparent - is that still creative? No? What about GPT-3 creating HTML code from a simple prompt, specifying what the user would like to have coded? It often times depends on where you draw the line, and the choice is often more arbitrary than one might think, when put to the test.

An interesting angle is added to this was presented in Lamb et al (2018) \cite{LambCarolyn2018ECCA}: “Consider this perspective: does your system model human creativity or do you want to make an argument that your system is creative because of the kinds of tasks it does”. In other words, simply replicating tasks which, if successfully carried out by humans, would constitute creativity, does not constitute creativity. Consider, for instance, deep neural networks such as “Deep Dream” \cite{DeepDream2015} or "MidJourney"\footnote{\url{https://www.midjourney.com/home/}}, which use deep neural networks to generate artworks of different styles, based on an input prompt. While the outputs of the model are often quite beautiful to look at, and would have doubtlessly been deemed creative, if created by a human, many would argue that they \textbf{aren't}, and they might have a point. The network has simply been trained on an enormous dataset of images, learning how to replicate different styles so, when it comes down to it, it’s just a convoluted transformation of RGB values. Regardless of where you land on this and similar issues, when push comes to shove, it is not a discussion of “rights” and “wrongs”, but one of opinions, based on one's own subjective experience, which is in realty quite emblematic for computational creativity.