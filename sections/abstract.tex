\label{chap:abstract}
\begin{abstract}
Within the last few decades, the field of computational creativity has bore witness to a number of new approaches with regards to mimicking creativity. In the field of NLP and specifically lyrics generation, both word- and line-based machine learning models of various types have been employed and proved successful in generating lyrics. However, the majority of these models have operated on the higher levels of language abstraction, both training and generating language-bites in the form of either lines or words. This has allowed models focus on the semantic quality of the resulting lyrics, without having to deal with syntax, in the case of line-based models, nor orthography. Arguably, this  results in the output being both less "creative" and certainly less original, for better or worse. In this paper, I present an exploratory analysis of rap lyrics generation, using a character-based Long Short-Term Memory (LSTM) neural network with a number of both deterministic and stochastic decoding algorithms. This is done first through an examination of the anatomy of rap lyrics, providing a foundation upon which to build the model and dataset. After this examination, the paper presents an extensive parameter optimization of the various decoding schemes, all of which are concurrently documented and analysed. Through this exploratory analysis, I find that the recently proposed \textbf{nucleus sampling} decoding algorithm is best scheme overall at generating \textbf{novel} and \textbf{coherent} text. Furthermore, using this scheme, I find that it is indeed possible to generate output syntactically and lexically similar to rap lyrics, using a fairly limited dataset on a relatively small character-based model. Throughout the study, both statistical metrics and concrete examples of the generated lyrics for the various decoding approaches is including, providing a full account of the model performance. The model and the various decoding configurations are also tested against a similarly trained baseline \textbf{general-purpose language model}. Here I find that the model trained on rap lyrics is significantly better at mimicking their style, than both the general-purpose model, as well as the \textbf{Wikitext-2} dataset. The paper concludes with as a discussion of results as well as other related topics such as creativity, the emergence of pseudo-creative systems, future prospects within the field and more.
\end{abstract}