\label{chap:relatedwork}
\section{Related Work}

\subsection{Creativity and NLG}
\label{sec:creativity+nlg}
Creativity and Natural Language Generation (NLG) are two closely related subjects, at least in (computer) theory, and in recent years the fields have bore witnessed to a number of very exciting advances within what has been dubbed “Computational Creativity”.
In Potash et al. (2015) \cite{potash-etal-2015-ghostwriter} they create and employ a LSTM language model named DeepBeat to generate rap lyrics, in a fashion similar to this paper. Their approach diverges from this one, however, in their use of a word-based model, as opposed to a character-based one, and also in their focus on ghost-writing. They design their model specifically to mimic individual artists’ styles as closely as possible. To meet this end, they employ a TFIDF-frequency similarity measure, calculated by using cosine similarity, as well as a metric called \textit{rhyme density}, originally conceived and developed by Malmi et al. (2016) \cite{Malmi_2016}. They show that, in comparison with baseline n-gram models, their LSTM model is substantially better at both generating novel, coherent lyrics, as well as imitating the style of the given artist, measured using rhyme density.

In Malmi et al. (2016) \cite{Malmi_2016} they present DopeLearning, a learning architecture employing a neural network in conjunction with the RankSVM algorithm, in an attempt to identify the next line of rap lyrics, amongst a set of 299 other candidate lines. This approach diverges from the previous in that it treats the generation of lyrics as an information retrieval task, as opposed to a token generation task, meaning that the novelty of their generation is to be found at the verse/song level, as they are, in essence, mixing an extensive library of existing lyrics. To conduct the prediction task, they feed their RankSVM model a number of pre-selected features such different version of BOW (Bag of Words) sets, a line length measure, end rhymes density and more, both independently and in unison. On top of these and other features, they also train a neural network to perform the prediction task, taking as input a single line and set of previous lines, half of these pairs being real continuations of the previous line, the other half being fake candidates.

Their RankSVM model finds the greatest success using all the aforementioned features in unison, measured by the highest average predicted rank for the correct next line, with an average score of $60.8/300$. Interestingly, almost identical performance is observed when only employing their FastFeatsNN5 feature set, consisting of the 5 fastest features to procure, plus their neural network extractor. Another interesting finding is that in terms of individual metrics, the one that achieved the best recall at ranks 1, 5 and 30, is their EndRhyme feature, which captures the number of matching vowel phonemes at the end of the previous line, further cementing the centrality of rhymes in rap lyrics.

Ultimately, their DeepBeat model is able to identify the correct next line amongst 299 other randomly selected lines, with 17\% accuracy, more than 50 times better random chance, and classifies the correct line in the top 30, 53\% of the time.

In Gonçalo (2012) \cite{Goncalo2012PoeTryMe} and \cite{GoncaloOliveiraHugo2014PGwP} we are introduced to the interactive poetry generation platform: PoeTryMe. This platform differs from the previously discussed approaches in that it does not rely on neural network structures for generation, but rather uses a template-based approach with customizable model parameters. Due to it being template-based, the generation is here based on a number of resources which are found within their “Relations Manager”, a "macro" module which handles the different stages of operations. The process starts off with selection of a random relation instance, e.g. $t=\{word_1,predicate,word_2\}$, based on a set of seed words. Then, the output is fed to the grammar module, which renders the predicate and that output is then in turn fed to the contextualize module, which is able to track the relation instances, which rendered each of the lines generated. After 30 iterations, the 30 lines will then be ranked according to their structural relevance to the chosen seeds, after which the n highest-ranked nodes are selected, producing very interesting and customizable pieces of poetry.

In Brown et al. (2020) \cite{BrownTomB2020LMaF} present one of the most, if not the most, advanced language models we have ever seen in, which is a continuation of the OpenAI team's previous model \textit{GPT-2}, meaning \textit{Generative Pre-trained Transformer}. They present a number of variants of the model, distinguished by their sizes, ranging from 125 million parameters for the smallest \textit{GPT-3 Small} model a staggering 175 \textit{billion} parameters for the largest model named \textit{GPT-3 175B} or just \textit{GPT-3}. They trained the model a number of different datasets, including \textit{Wikipedia} (3b tokens/words), two book corpora (12b and 55b tokens respectively) and, lastly, a common crawl dataset of text scraped from all over the web, consisting of an inprecedented \textbf{410 billion} tokens.

In the paper, they explore a number of ways tuning the model, namely \textit{Zero-Shot} tuning (0S), \textit{One-Shot} tuning (1S) and \textit{Few-Shot} tuning (FS). Through testing with various tasks such \textit{machine translation} and downstream tasks (e.g. removing superfluous symbols from words) and other NLP tasks, they find FS to be the best performing, indivdually and on aggregate, especially as the number of parameters increase. They furthermore show hitherto unprecedented performance on a number of tasks, such as automatic generation of samples of news articles, as is shown by the difficulty of human evaluators in distinguishing between these and articles written by humans.

\subsection{Text Degeneration}
\label{sec:text-degeneration}
Text degeneration is an emerging subject of research, within the field of neural text generation, and specifically concerns the drawbacks of likelihood estimation as the training objective. While MLE is more or less universally used for training generative models, it has been shown to be flawed metric in many ways when it comes to accurately modelling \textit{real} human language, as it often gets stuck in loops of generating continuously repeating lines/sentences. Furthermore, contrary to previous beliefs, this issue has not been solved by even the most cutting-edge models such as GPT2 and more recently GPT3, suggesting that it is unlikely to be solved yet larger models in the future.

In Welleck et al. (2019) \cite{Welleck2019Unlikelyhood}, they suggest a new objective for training, namely \textit{un}likelihood training. It departs from the traditional likelihood training in that it forces unlikely generations to be assigned a lower probability by the model. Additionally, these unlikely tokens can be retrieved both during next-token prediction or from generated sequences, allowing the unlikelihood objective to be employed by both character- and word-based models. They show that for both automatic evaluation and human evaluations, models trained with the unlikelihood objective, outperformed current state-of-the-art models, especially in terms of degeneration i.e. repetition.

Another paper which aimed to combat language degeneration is Holtzmann et al. (2019) \cite{HoltzmanAri2019TCCo}, which proposes a now widely adopted decoding scheme called \textit{nucleus sampling} or sometimes \textit{top-p sampling}\footnote{ As an aside, these two terms will be used interchangeably throughout the paper.}. In the paper, they discuss how commonly used decoding schemes such as beam sampling, runs into the issues of a) generating repetitive text bites and b) being distributionally very dissimilar to how we use language when we communicate, as was also discussed in \cite{Welleck2019Unlikelyhood}. They show that previous sampling techniques are too probable in comparison with gold (human) standards, meaning that generative models too confidently, and therefore too often, predicts what they deem to be most likely \textit{next token}, in comparison with how likely said token would be to be picked in human communication, a bias which unfortunately inherent to how the MLE objective functions. As a consequence of this, they observe that for any MLE-optimized model, the probability of generating repeated phrases often increased with each iteration, creating a positive feedback loop for repetition.

In the the proposal of nucleus sampling, they draw upon the successes of top-k sampling, presented in Fan et al. (2018) \cite{FanAngela2018HNSG}, improving upon the structure by re-furnishing it to better adapt to dynamic distributions. They do this by simply swapping the static \textit{k} variable with the more adaptive \textit{p}. Where top-k sampling re-distributes from the best \textbf{k} number of options, nucleus sampling uses the top $p$ percentage of options, \textit{p} denoting the size of the cumulative probability mass which can be filled by the smallest possible subset. Using a range of different metrics such as perplexity, repetition percentage, BLUE4 and more, they test test their new decoding method and they show that nucleus sampling outperforms both other decoding methods on the given metrics, while also being the overall at mimicking human language production in terms of word distribution.

\subsection{Generated Text Evaluation}
\label{sec:eval-of-generated-text}
Lippi et al. (2019) \cite{LippiMarco2019NLSF} explores and expands upon the evaluative scope of NLG models, in particular LSTMs and Markov models of various orders, creating an interesting evaluative framework. The framework incorporates a number of promising evaluation metrics such as long-range correlation measures, KL-divergence estimation, creativity, authorship attributions methods and more. Employing this structure on the aforementioned models, they confirm the effectiveness of LSTM models at mapping long-range dependencies, as well as their ability to emulate the Zipf’ian distributions of language, with the right parameter tuning (using \textit{temperature} and \textit{top-k} sampling).

In Potash et al. (2016) \cite{PotashPeter2016ECLG} they expand upon their previous findings \cite{potash-etal-2015-ghostwriter}, this time employing a mixture of both automatic as well as manual human evaluation, and generally expanding upon their evaluative framework. Furthermore, they increase the number of artists for their ghost-writing task from 1 to 11, selecting both artists with comparatively small, medium, and large vocabularies. They find further support for their previous finding using inner-annotator agreement measures to evaluate the fluency of the generated lyrics for individual rappers, and also find success in fully automating their previously semi-automated evaluation methods.

In Lamb et al. (2018) \cite{LambCarolyn2018ECCA}, they examine and discuss the evaluation of creative system by drawing upon knowledge from multiple fields that involve creativity. In doing so, they propose a range of questions to be considered, prior to creating a computational creativity system. Amongst the questions/inquiries which were most relevant in this current paper's framework were: “If using novelty and value, consider what kind of novelty the product should have.” Here, this meant assessing to whether the novelty would emerge \textit{just} from being able to generate plausible rap lyrics using a character-based approach, or whether the bar should be set higher. Another set of considerations was prompted by the recommendation to “be careful [in] distinguish[ing] between meaningful novelty and randomness.” For this paper, it meant considering how to distinguish between the “novelty” reflected by some of the metrics employed in the parameter tuning stage (mainly \hyperref[para:avg-unique-words/v]{AUW} and \hyperref[para:total-vocab-size]{VS}), as we will discuss further later on. It was also useful to keep in mind, in performing manual evaluation on the generated lyrics, as the common emergence of entertaining words or word combination (refridgeraneous being amongst the best ones) were mostly based upon the randomness of the given sampling method. Overall, the paper presents several valuable insights with regards to creativity, providing a comprehensive interdisciplinary checklist of sorts, for anyone working on and/or dealing with computationally creative systems.