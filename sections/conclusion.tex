\section{Conclusion}
\label{chap:conclusion}

To conclude, in this paper I explored a number of different decoding techniques for automatic language generation, namely greedy search, beam search, temperature sampling, top-k sampling and nucleus sampling. Through my extensive exploration of different parameter configurations, I managed to find a suitable range of locally optimal parameter values for each of the given techniques, within our setup. I also tested the stylistic validity of the generated text as rap lyrics. This was done mainly by computing the rhyme density for each configuration, as a means of gathering their similarity in style with our rap lyrics dataset, an approach which has proven succesfull in a number of previous studies. I also extended the evaluation by testing against other baselines such text generated by GP model, which was also created and trained during the course of the study, as well as the general-purpose dataset upon which the GP model was trained.

\textbf{This analysis showed that the it is indeed possible to create a LM which can fairly successfully generate rap verses, even using only a limited dataset as well as a limited set of evaluation metrics}. All of the parameter configurations for my RL model succeeded in beating the worst rappers in terms of rhyme density, some even approaching the middle of the pack (top-k, $k=2, t=0.6$).

While previous studies \cite{PotashPeter2016ECLG} \cite{potash-etal-2015-ghostwriter} \cite{Malmi_2016} has shown the ability to do so using a word-based generation models and line-based retrieval models, I here demonstrate the ability to accomplish similar results with a character-based model. It did, however, also showcase issues for some configurations with maintaining correct spelling, presenting a clear trade-off in terms of legibility, novelty and stylistic similarity. This can be seen in both the \hyperref[sec:gen+eval]{real average word length} vs. the dataset, as well as the \hyperref[fig:non-words]{number of non-words}, both of which proved lackluster. However, in comparison with previous attempts, the task is substantially more demanding with regards to learning the text, as the model has to grapple with not only the distribution of words, but also the creation of correctly spelled words, which then has to be strung together in a coherent manner.

Determining which of our configurations performed the best, given the previously mentioned trade-offs between them, is not a straight forward task. \textbf{Given the entire field of results gathered by examining the metrics, I judge the nucleus/top-p sampling configurations to be the most successful}. This decision was primarily influenced by, for the top-k configurations, the inability to consistently produce adequate text in terms of readability and, for the other configurations, the issue(s) of either repetition or lack of novelty. An honorable mentioned should be granted the temperature sampling scheme, which in many ways outperformed the closest runner-up in the top-k scheme, despite its being an inherently simpler version.