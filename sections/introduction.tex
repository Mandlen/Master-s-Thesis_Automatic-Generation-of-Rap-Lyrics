\label{chap:introduction}
\section{Introduction}

While many might consider individuality an irreplaceable cornerstone of creating literature, be it book writing, speech composition or songwriting, ghost-writing nevertheless remains a widespread phenomenon. In the field of music, this has historically been a function carried out by up-and-coming singer-songwriters, as a way of gaining recognition and connections. Recently, however, the music sphere has bore witness to the emergence of computational creativity systems, able to provide comparable assistance both semi- and fully automatically. Such systems are generally made up of various types of machine learning models, the current reigning champion being neural network language models (NNLMs). These powerful models have shown the prowess in a variety of tasks such as conversational mapping and generation \cite{VinyalsOriol2015ANCM}, image labelling \cite{Egmont-PetersenM.2002Ipwn} scientific paper summaries \cite{CollinsEd2017ASAt} programming in C++ \cite{karpathy_2015} and many, many more \cite{BrownTomB2020LMaF}. The emergence of such system has presented both researchers and audiences alike with a long list of questions regarding creativity. Can computer systems produce \textit{real} creative content, and if so, how do we distinguish between creativity and other? Where does one draw the between the eventual luck of the proverbial monkey with a typewriter and the seemingly inventive merging of existing creative material? Is the creative process even important, or should the focus instead be directed at the quality of the product? If we cannot ultimately tell the difference between human- and AI generated material, does it even matter? Is creativity even a characteristic that is able to be assigned to automatic systems or does it inherently require human intervention? If so, what about semi-automatic creative systems? How does prompt a creative process from a agent whose actions depend on our specifications, and therefore are, in some sense, predetermined? And lastly, and most importantly, what actually \textit{is} creativity and what does it entail?
This is seemingly a question best left for the philosophers and psychologists, but while we might not be able to present a satisfactory answer to the question, it nevertheless presents us with an interesting line of inquiries, for how we choose to design such systems that attempt to replicate the creativity.

In this paper, I will be implementing a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) with the goal of generating at rap lyrics that are both novel and coherent, and also follow the general style of rap lyrics. To this end, I will be training my model on a corpus of rap lyrics from 37 different rap artists. Furthermore, I plan to explore the often overlooked area of parameter optimization, specifically with regards to LM decoding strategies, employing a number of different metrics to measure the efficiency and effectiveness of different decoding+parameter configurations. This exploration will be examining 5 different decoding strategies and covering more than 120 different parameter configurations in total, as to give the results gathered in the later generation stage more authority.

After the parameter tuning process, I plan to re-test all the top-scoring configurations gathered from the previous stage, on a new set of more specialized metrics, each with a greater volume of generated lyrics, as to better gauge the validity of each configurationâ€™s performance. Lastly, the top configurations will be evaluated based on the final and most subject-specific metric, namely rhyme density, to measure how well our model is able to re-create rap lyrics. For this purpose, I will also be training another \textbf{General-purpose} model on the same LSTM architecture used for the lyrics model using a general-purpose dataset. This model will be used to generate text as to establish a second \textit{model baseline} against which to compare the lyrics model's ability to mimic the style of rap lyrics specifically. 

To summarize, I will be attempting to answer the question of whether a character-based language model of limited size is able to generate of rap lyrics, defined as text that imitates lyrics written by real rappers.

The paper will be organised as follows: I start out with a brief overview of related work, focusing primarily, but not exclusively, on the topics of \hyperref[sec:creativity+nlg]{creativity}, \hyperref[sec:text-degeneration]{text degeneration} and \hyperref[sec:eval-of-generated-text]{evaluation of generated text}. Next I will give a brief overview of \hyperref[sec:modelling-rap]{how to model rap lyrics}, which is comprised of an \hyperref[sec:anatomy-of-rap]{anatomy of rap lyrics} as well as a section on \hyperref[sec:design-adaptation]{designating the model}, focusing on applying the knowledge of rap lyrics to our data and model. Next, I will go over my methodology which consists of an overview of the chosen \hyperref[sec:datasets]{datasets}, our \hyperref[sec:pre-processing]{pre-processing} steps, and our \hyperref[sec:generation-pipeline]{generation-pipeline}. Afterwards, I will cover our \hyperref[sec:setup+eval]{setup and evaluation} methods as well as the metrics used, followed by and overview of our \hyperref[sec:lstm-model]{LSTM model(s)}, \hyperref[sec:model-architecture]{the architecture}, \hyperref[sec:model-training]{training process(es)} etc. Lastly, I will finish off the methodology by outlining our choice of \hyperref[sec:generation-techs]{generation techniques and decoding algorithms}.

Following that, we will move on to our \hyperref[chap:tuning-params]{parameter tuning }. This section will focus on tuning process of the various decoding algorithms. Afterwards, we will be diving into the \hyperref[sec:gen+eval]{results}, looking at the generation and evaluation of lyrics. These sections will both be comprehensive in terms of covering not only the results of our tests, but also including analyses regarding the process of parameter tuning, the significance and interpretation of different results, etc.

After our results, we will move on to the \hyperref[chap:discussion]{discussion}, in which I will firstly provide an assessment of the gathered results, commenting on the most important findings, as well as proposing solutions for potential problems. Then, I will go over the general \hyperref[sec:limitation+future-work]{limitations} of our model and \hyperref[sec:extending-evaluation]{possible improvements} with regards to our as a whole study, also including the potential for \hyperref[sec:transformers-etc]{future work} within the field as a whole. Additionally, the discussion will also contain some exploration on the topic of \hyperref[sec:creativity+language-patterns]{computational creativity}, scrutinizing the subject of creativity and originality. Finally, I will wrap up the findings in the \hyperref[chap:conclusion]{conclusion}, discussing my initial aspirations, the significance and scope of the results and what we can take away from the findings. The rest of the paper will consist of the references and the appendix, in which you can find the graphs and tables used in the making of the paper.
